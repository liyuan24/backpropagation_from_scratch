{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "* http://neuralnetworksanddeeplearning.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.82417516, -0.2403351 , -0.64042118, -1.6091103 ],\n",
       "       [-0.37115847, -1.58796884, -1.74194645,  0.75002029],\n",
       "       [-1.25362636, -0.09506361,  2.42418629,  1.23684067]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "\n",
    "class Network():\n",
    "    def __init__(self, sizes: list[int]):\n",
    "        '''\n",
    "        sizes: specifies the number of neurons each layer\n",
    "        '''\n",
    "        # [0, 1] uniformly random initialization is not a good idea, because you weights are all positive numbers \n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            x = sigmoid(np.dot(w, x) + b)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def SGD(self, training_data, epochs, batch_size, lr, test_data=None):\n",
    "        '''\n",
    "        train the neutral network with SGD\n",
    "        training_data: a list of tuple of (x, y) where x is of shape (size[0], 1) and y is of shape(size[-1], 1)\n",
    "        epochs: the number of epochs of training\n",
    "        batch_size: the mini-batch size\n",
    "        lr: learning rate\n",
    "        test_data: if provided, we will also evaluate on the test_data after each epoch\n",
    "        '''\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_minibatch(mini_batch, lr)\n",
    "            if test_data:\n",
    "                correct_cnt = self.evaluate(test_data)\n",
    "                print(f\"Epoch {epoch}, classification accuracy is {correct_cnt} / {n_test}\")\n",
    "            \n",
    "        \n",
    "    \n",
    "    def update_minibatch(self, minibatch, lr):\n",
    "        '''\n",
    "        Update the weights and bias after backprop for each minibatch\n",
    "        '''\n",
    "        weight_updates = [np.zeros(weight.shape) for weight in self.weights]\n",
    "        bias_updates = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        for x, y in minibatch:\n",
    "            weight_deltas, bias_deltas = self.backprop(x, y)\n",
    "            weight_updates = [w_update + w_delta for w_update, w_delta in zip(weight_updates, weight_deltas)]\n",
    "            bias_updates = [b_update + b_delta for b_update, b_delta in zip(bias_updates, bias_deltas)]\n",
    "        \n",
    "        self.weights = [w - lr * weight_update / len(minibatch) for w, weight_update in zip(self.weights, weight_updates)]\n",
    "        self.biases = [b - lr * bias_update / len(minibatch) for b, bias_update in zip(self.biases, bias_updates)]\n",
    "        \n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        '''\n",
    "        x: the input of the neutral network of shape [size[0], 1]\n",
    "        y: the label of size [size[-1], 1]\n",
    "        It only deals with one training example. The aggregation will be handled in update_mini_batch()\n",
    "        First do a feed forward pass to get the prediction and then do the backpropagation layer by layer\n",
    "        return the weights and bias updates layer by layer\n",
    "        '''\n",
    "        # we need to store the activations to get the Jacobian of neutrons of next layer wrt to weights\n",
    "        # used to get derivative wrt weights\n",
    "        activations = [x]\n",
    "        # we also need to store the values before activation. This is used to get the derivative wrt to activation functions\n",
    "        # like sigmoid's derivative is sigmoid(x) * (1-sigmoid(x)) where x is the value before applying sigmoid activation\n",
    "        zs = []\n",
    "        activation = x\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # the partial derivative wrt the activations of shape [size[-1], 1], it will be passed down\n",
    "        partial_deriv_activation = self.cost_deriv(activation, y) * sigmoid_prime(zs[-1])\n",
    "        weights_deltas = [np.zeros(w.shape) for w in self.weights]\n",
    "        bias_deltas = [np.zeros(b.shape) for b in self.biases]\n",
    "        # there are self.num_layers - 1 weights and bias\n",
    "        for i in range(1, self.num_layers):\n",
    "            # first matrix multiplication in backprop at each step, partial_deriv_activation * Jacobaian wrt weights\n",
    "            weights_deltas[-i] = np.dot(partial_deriv_activation, activations[-(i+1)].transpose())\n",
    "            bias_deltas[-i] = partial_deriv_activation\n",
    "            # second matrix multiplication in backprop at each step, partial_deriv_activation * Jacobian wrt node values in prev layer\n",
    "            # we can see that this derivative is passed backward and multiplied together\n",
    "            # this is where vanishing or exploding gradients come from\n",
    "            if i < self.num_layers-1:\n",
    "                partial_deriv_activation = np.dot(self.weights[-i].transpose(), partial_deriv_activation) * sigmoid_prime(zs[-(i+1)])\n",
    "        return (weights_deltas, bias_deltas)\n",
    "            \n",
    "    def evaluate(self, test_data) -> int:\n",
    "        '''\n",
    "        test_data: a list of tuple (x, y) where x is of shape [size[0], 1] and y is an integer ranging from 0 to 9\n",
    "        '''\n",
    "        res = [(np.argmax(self.forward(x)), y) for x, y in test_data]\n",
    "        return sum([int(x == y) for x, y in res])\n",
    "        \n",
    "    # this is the detrivative of mean squared error\n",
    "    def cost_deriv(self, pred, y):\n",
    "        return (pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    file_path = './data/mnist.pkl.gz'\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        # the default encoding is ascii, we need to use latin1 to avoid error when decoding\n",
    "        training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    train_data, val_data, test_data = load_data()\n",
    "    train_data_input = [x.reshape(784, 1) for x in train_data[0]]\n",
    "    train_data_labels = [one_hot_encoding(y) for y in train_data[1]]\n",
    "    train_data = list(zip(train_data_input, train_data_labels))\n",
    "    val_data_input = [x.reshape(784, 1) for x in val_data[0]]\n",
    "    val_data = list(zip(val_data_input, val_data[1]))\n",
    "    test_data_input = [x.reshape(784, 1) for x in test_data[0]]\n",
    "    test_data = list(zip(test_data_input, test_data[1]))\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def one_hot_encoding(x: int):\n",
    "    res = np.zeros((10, 1), dtype='int')\n",
    "    res[x] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = load_data_wrapper()\n",
    "# train_data[0][0].shape, train_data[1].shape, val_data[0].shape, val_data[1].shape, test_data[0].shape, test_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 1), (10, 1), (784, 1), (), (784, 1), ())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape, train_data[0][1].shape, val_data[0][0].shape, val_data[0][1].shape, test_data[0][0].shape, test_data[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "With only one hidden layer, the best classification accuracy on the test dataset could be above 90%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network([784, 30, 10])\n",
    "# hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "lr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, classification accuracy is 7266 / 10000\n",
      "Epoch 1, classification accuracy is 7784 / 10000\n",
      "Epoch 2, classification accuracy is 7951 / 10000\n",
      "Epoch 3, classification accuracy is 8060 / 10000\n",
      "Epoch 4, classification accuracy is 8145 / 10000\n",
      "Epoch 5, classification accuracy is 8899 / 10000\n",
      "Epoch 6, classification accuracy is 8969 / 10000\n",
      "Epoch 7, classification accuracy is 9039 / 10000\n",
      "Epoch 8, classification accuracy is 9086 / 10000\n",
      "Epoch 9, classification accuracy is 9129 / 10000\n"
     ]
    }
   ],
   "source": [
    "network.SGD(train_data, epochs, batch_size, lr, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Network\n",
    "1. Weight initialization with scaled factor of $1/\\sqrt{n_{in}}$\n",
    "2. Weight decay regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "\n",
    "class Network2():\n",
    "    def __init__(self, sizes: list[int]):\n",
    "        '''\n",
    "        sizes: specifies the number of neurons each layer\n",
    "        '''\n",
    "        # random normal distribution with standard deviation adjustment\n",
    "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            x = sigmoid(np.dot(w, x) + b)\n",
    "        # use softmax in the final layer activation\n",
    "        return softmax(np.dot(self.weights[-1], x) + self.biases[-1])\n",
    "\n",
    "\n",
    "    def SGD(self, training_data, epochs, batch_size, lr, weight_decay= 0.0, test_data=None):\n",
    "        '''\n",
    "        train the neutral network with SGD\n",
    "        training_data: a list of tuple of (x, y) where x is of shape (size[0], 1) and y is of shape(size[-1], 1)\n",
    "        epochs: the number of epochs of training\n",
    "        batch_size: the mini-batch size\n",
    "        lr: learning rate\n",
    "        test_data: if provided, we will also evaluate on the test_data after each epoch\n",
    "        '''\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_minibatch(mini_batch, lr, n, weight_decay)\n",
    "            if test_data:\n",
    "                correct_cnt = self.evaluate(test_data)\n",
    "                print(f\"Epoch {epoch}, classification accuracy is {correct_cnt} / {n_test}\")\n",
    "            \n",
    "        \n",
    "    \n",
    "    def update_minibatch(self, minibatch, lr, n, weight_decay):\n",
    "        '''\n",
    "        Update the weights and bias after backprop for each minibatch\n",
    "        '''\n",
    "        weight_updates = [np.zeros(weight.shape) for weight in self.weights]\n",
    "        bias_updates = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        for x, y in minibatch:\n",
    "            weight_deltas, bias_deltas = self.backprop(x, y)\n",
    "            weight_updates = [w_update + w_delta for w_update, w_delta in zip(weight_updates, weight_deltas)]\n",
    "            bias_updates = [b_update + b_delta for b_update, b_delta in zip(bias_updates, bias_deltas)]\n",
    "        \n",
    "        self.weights = [(1-lr * weight_decay / n) * w - lr * weight_update / len(minibatch) for w, weight_update in zip(self.weights, weight_updates)]\n",
    "        self.biases = [b - lr * bias_update / len(minibatch) for b, bias_update in zip(self.biases, bias_updates)]\n",
    "        \n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        '''\n",
    "        x: the input of the neutral network of shape [size[0], 1]\n",
    "        y: the label of size [size[-1], 1]\n",
    "        It only deals with one training example. The aggregation will be handled in update_mini_batch()\n",
    "        First do a feed forward pass to get the prediction and then do the backpropagation layer by layer\n",
    "        return the weights and bias updates layer by layer\n",
    "        '''\n",
    "        # we need to store the activations to get the Jacobian of neutrons of next layer wrt to weights\n",
    "        # used to get derivative wrt weights\n",
    "        activations = [x]\n",
    "        # we also need to store the values before activation. This is used to get the derivative wrt to activation functions\n",
    "        # like sigmoid's derivative is sigmoid(x) * (1-sigmoid(x)) where x is the value before applying sigmoid activation\n",
    "        zs = []\n",
    "        activation = x\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # the partial derivative wrt the activations of shape [size[-1], 1], it will be passed down\n",
    "        partial_deriv_activation = (activation - y)\n",
    "        weights_deltas = [np.zeros(w.shape) for w in self.weights]\n",
    "        bias_deltas = [np.zeros(b.shape) for b in self.biases]\n",
    "        # there are self.num_layers - 1 weights and bias\n",
    "        for i in range(1, self.num_layers):\n",
    "            # first matrix multiplication in backprop at each step, partial_deriv_activation * Jacobaian wrt weights\n",
    "            weights_deltas[-i] = np.dot(partial_deriv_activation, activations[-(i+1)].transpose())\n",
    "            bias_deltas[-i] = partial_deriv_activation\n",
    "            # second matrix multiplication in backprop at each step, partial_deriv_activation * Jacobian wrt node values in prev layer\n",
    "            # we can see that this derivative is passed backward and multiplied together\n",
    "            # this is where vanishing or exploding gradients come from\n",
    "            if i < self.num_layers-1:\n",
    "                partial_deriv_activation = np.dot(self.weights[-i].transpose(), partial_deriv_activation) * sigmoid_prime(zs[-(i+1)])\n",
    "        return (weights_deltas, bias_deltas)\n",
    "            \n",
    "    def evaluate(self, test_data) -> int:\n",
    "        '''\n",
    "        test_data: a list of tuple (x, y) where x is of shape [size[0], 1] and y is an integer ranging from 0 to 9\n",
    "        '''\n",
    "        res = [(np.argmax(self.forward(x)), y) for x, y in test_data]\n",
    "        return sum([int(x == y) for x, y in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "network2 = Network2([784, 30, 10])\n",
    "# hyperparameters\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "lr = 3\n",
    "weight_decay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, classification accuracy is 9119 / 10000\n",
      "Epoch 1, classification accuracy is 9274 / 10000\n",
      "Epoch 2, classification accuracy is 9301 / 10000\n",
      "Epoch 3, classification accuracy is 9398 / 10000\n",
      "Epoch 4, classification accuracy is 9446 / 10000\n",
      "Epoch 5, classification accuracy is 9406 / 10000\n",
      "Epoch 6, classification accuracy is 9447 / 10000\n",
      "Epoch 7, classification accuracy is 9475 / 10000\n",
      "Epoch 8, classification accuracy is 9447 / 10000\n",
      "Epoch 9, classification accuracy is 9503 / 10000\n",
      "Epoch 10, classification accuracy is 9453 / 10000\n",
      "Epoch 11, classification accuracy is 9509 / 10000\n",
      "Epoch 12, classification accuracy is 9462 / 10000\n",
      "Epoch 13, classification accuracy is 9528 / 10000\n",
      "Epoch 14, classification accuracy is 9469 / 10000\n",
      "Epoch 15, classification accuracy is 9512 / 10000\n",
      "Epoch 16, classification accuracy is 9520 / 10000\n",
      "Epoch 17, classification accuracy is 9514 / 10000\n",
      "Epoch 18, classification accuracy is 9514 / 10000\n",
      "Epoch 19, classification accuracy is 9513 / 10000\n",
      "Epoch 20, classification accuracy is 9532 / 10000\n",
      "Epoch 21, classification accuracy is 9521 / 10000\n",
      "Epoch 22, classification accuracy is 9537 / 10000\n",
      "Epoch 23, classification accuracy is 9504 / 10000\n",
      "Epoch 24, classification accuracy is 9533 / 10000\n",
      "Epoch 25, classification accuracy is 9520 / 10000\n",
      "Epoch 26, classification accuracy is 9535 / 10000\n",
      "Epoch 27, classification accuracy is 9541 / 10000\n",
      "Epoch 28, classification accuracy is 9536 / 10000\n",
      "Epoch 29, classification accuracy is 9542 / 10000\n"
     ]
    }
   ],
   "source": [
    "network2.SGD(train_data, epochs, batch_size, lr, weight_decay, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, classification accuracy is 9285 / 10000\n",
      "Epoch 1, classification accuracy is 9361 / 10000\n",
      "Epoch 2, classification accuracy is 9299 / 10000\n",
      "Epoch 3, classification accuracy is 9511 / 10000\n",
      "Epoch 4, classification accuracy is 9479 / 10000\n",
      "Epoch 5, classification accuracy is 9536 / 10000\n",
      "Epoch 6, classification accuracy is 9540 / 10000\n",
      "Epoch 7, classification accuracy is 9532 / 10000\n",
      "Epoch 8, classification accuracy is 9525 / 10000\n",
      "Epoch 9, classification accuracy is 9550 / 10000\n",
      "Epoch 10, classification accuracy is 9543 / 10000\n",
      "Epoch 11, classification accuracy is 9560 / 10000\n",
      "Epoch 12, classification accuracy is 9587 / 10000\n",
      "Epoch 13, classification accuracy is 9594 / 10000\n",
      "Epoch 14, classification accuracy is 9574 / 10000\n",
      "Epoch 15, classification accuracy is 9594 / 10000\n",
      "Epoch 16, classification accuracy is 9597 / 10000\n",
      "Epoch 17, classification accuracy is 9565 / 10000\n",
      "Epoch 18, classification accuracy is 9591 / 10000\n",
      "Epoch 19, classification accuracy is 9604 / 10000\n",
      "Epoch 20, classification accuracy is 9600 / 10000\n",
      "Epoch 21, classification accuracy is 9606 / 10000\n",
      "Epoch 22, classification accuracy is 9592 / 10000\n",
      "Epoch 23, classification accuracy is 9610 / 10000\n",
      "Epoch 24, classification accuracy is 9603 / 10000\n",
      "Epoch 25, classification accuracy is 9586 / 10000\n",
      "Epoch 26, classification accuracy is 9579 / 10000\n",
      "Epoch 27, classification accuracy is 9600 / 10000\n",
      "Epoch 28, classification accuracy is 9602 / 10000\n",
      "Epoch 29, classification accuracy is 9585 / 10000\n"
     ]
    }
   ],
   "source": [
    "network2 = Network2([784, 30, 10])\n",
    "# hyperparameters\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "lr = 3\n",
    "weight_decay = 1\n",
    "network2.SGD(train_data, epochs, batch_size, lr, weight_decay, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, classification accuracy is 9132 / 10000\n",
      "Epoch 1, classification accuracy is 9192 / 10000\n",
      "Epoch 2, classification accuracy is 9278 / 10000\n",
      "Epoch 3, classification accuracy is 9307 / 10000\n",
      "Epoch 4, classification accuracy is 9253 / 10000\n",
      "Epoch 5, classification accuracy is 9382 / 10000\n",
      "Epoch 6, classification accuracy is 9381 / 10000\n",
      "Epoch 7, classification accuracy is 9385 / 10000\n",
      "Epoch 8, classification accuracy is 9435 / 10000\n",
      "Epoch 9, classification accuracy is 9444 / 10000\n",
      "Epoch 10, classification accuracy is 9406 / 10000\n",
      "Epoch 11, classification accuracy is 9513 / 10000\n",
      "Epoch 12, classification accuracy is 9520 / 10000\n",
      "Epoch 13, classification accuracy is 9517 / 10000\n",
      "Epoch 14, classification accuracy is 9535 / 10000\n",
      "Epoch 15, classification accuracy is 9523 / 10000\n",
      "Epoch 16, classification accuracy is 9573 / 10000\n",
      "Epoch 17, classification accuracy is 9532 / 10000\n",
      "Epoch 18, classification accuracy is 9548 / 10000\n",
      "Epoch 19, classification accuracy is 9597 / 10000\n",
      "Epoch 20, classification accuracy is 9529 / 10000\n",
      "Epoch 21, classification accuracy is 9580 / 10000\n",
      "Epoch 22, classification accuracy is 9551 / 10000\n",
      "Epoch 23, classification accuracy is 9584 / 10000\n",
      "Epoch 24, classification accuracy is 9582 / 10000\n",
      "Epoch 25, classification accuracy is 9590 / 10000\n",
      "Epoch 26, classification accuracy is 9544 / 10000\n",
      "Epoch 27, classification accuracy is 9605 / 10000\n",
      "Epoch 28, classification accuracy is 9575 / 10000\n",
      "Epoch 29, classification accuracy is 9629 / 10000\n"
     ]
    }
   ],
   "source": [
    "network2 = Network2([784, 30, 10])\n",
    "# hyperparameters\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "lr = 3\n",
    "weight_decay = 5\n",
    "network2.SGD(train_data, epochs, batch_size, lr, weight_decay, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
